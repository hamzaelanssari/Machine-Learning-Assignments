{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://www.dates-concours.ma/wp-content/uploads/2019/05/ENSET-Mohemmedia-300x141.png\" width=\"300\" alt=\"ENSET logo\"  />\n",
    "</center>\n",
    "\n",
    "# Multi-Layer Network from scratch: Devoir \n",
    "### By Hamza El Anssari \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "L’objectif de trouver un modèle pour la séparation d’un XOR généralisé comme montré dans la figure\n",
    "suivante. (voir atelier « 3.Multi Layer NN - XOR problem »\n",
    "Dans la démonstration, utiliser le même dataset utilisé dans #program2 de l’atelier « 3.Multi Layer NN -\n",
    "XOR problem »"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of content</h2>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "En se basant sur les codes sources présentés dans les cours et des recherches à effectuer, modéliser et\n",
    "implémenter une application qui créé un réseau de neurones à trois couches (couche d’entrée, couche\n",
    "cachée, couche de sortie). Cette application prend en considération les éléments suivants :\n",
    "<ul>\n",
    "    <li><a>Packages</a></li>\n",
    "    <li>Créer une classe MultiLayerNN qui contient :</li>\n",
    "    <ul>\n",
    "    <li><a>Un constructeur pour initialiser le réseau de neurones</a></li>\n",
    "    <li><a>Une méthode fit pour l’apprentissage basé sur l’Algorithme gradient Descent pour assurer l’apprentissage des poids</a></li>\n",
    "    <li><a>Une méthode predict pour faire la prédiction</a></li>\n",
    "    </ul>\n",
    "    <li>L’application doit être paramétrable de la manière suivante :</li>\n",
    "    <ul>\n",
    "    <li><a>Définir le nombre de neurones de la couche cachée</a></li>\n",
    "    <li><a>Définir le nombre de neurones de la couche output</a></li>\n",
    "    <li><a>Préciser les fonctions d’activations</a></li>\n",
    "    <li><a>Préciser le nombre d’epochs</a></li>\n",
    "    <li><a>Préciser le learning rate</a></li>\n",
    "    </ul>\n",
    "    <li>En cas de besoin, vous pouvez définir autres classes et autres fonctions</li>\n",
    "</ul>\n",
    "    \n",
    "</div>\n",
    " \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>1 - Packages</font> ###\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. \n",
    "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- [seaborn](http://seaborn.pydata.org/) is a library that uses Matplotlib underneath to plot graphs. \n",
    "- [pandas](http://pandas.pydata.org/) is a library for data analysis and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>2 - Multi Layer NN - XOR problem</font> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Sigmoid Function & Sigmoid Derivative & Loss Function :**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "# Sigmoid Derivative\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Loss Function\n",
    "def loss(yp,y):\n",
    "    return (1/2)*np.square(yp-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Data Initialization :**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(300, 2)\n",
    "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Neural Network Initialization :**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input datasets\n",
    "inputs = X \n",
    "expected_output = y.reshape(300,1)\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.001\n",
    "losses = []\n",
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,4,1\n",
    "\n",
    "#Random weights and bias initialization\n",
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))\n",
    "hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
    "output_bias = np.random.uniform(size=(1,outputLayerNeurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 - Forward Function :**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, hidden_weights,  hidden_bias, output_weights, output_bias):\n",
    "    # Hidden Layer\n",
    "    hidden_layer_activation = np.dot(inputs,hidden_weights) + hidden_bias\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "    \n",
    "    # Output Layer\n",
    "    output_layer_activation = np.dot(hidden_layer_output,output_weights) + output_bias\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "    return hidden_layer_output, predicted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 - Backward Function :**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 - Fit Model Function :**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(expected_output, predicted_output, output_weights):\n",
    "    # Output Layer\n",
    "    error = expected_output - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    # Hidden Layer\n",
    "    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    return d_hidden_layer, d_predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, inputs, hidden_weights,  hidden_bias, output_weights, output_bias):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    #Training algorithm\n",
    "    for _ in range(epochs):\n",
    "        #Forward Propagation\n",
    "        hidden_layer_output, predicted_output = forward(inputs, hidden_weights,  hidden_bias, output_weights, output_bias)\n",
    "\n",
    "        #Backpropagation\n",
    "        d_hidden_layer, d_predicted_output = backward(expected_output, predicted_output, output_weights)\n",
    "\n",
    "        #Updating Weights and Biases\n",
    "        output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "        output_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr\n",
    "        hidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "        hidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
    "\n",
    "        #Loss\n",
    "        # loss_ = loss(expected_output, predicted_output)[0]\n",
    "        # losses.append(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7 - Predict Function :**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, hidden_weights, hidden_bias, output_weights, output_bias):\n",
    "    predicted_output = forward(inputs, hidden_weights,  hidden_bias, output_weights, output_bias)[1]\n",
    "    predicted_output = np.squeeze(predicted_output)\n",
    "    if predicted_output>=0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
